{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (RAG) system built from scratch using Sentence-Transformers, FAISS, Cross-Encoder reranking, and an open-source LLM"
      ],
      "metadata": {
        "id": "04c9Gqg_3Jgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies & Imports & Setup"
      ],
      "metadata": {
        "id": "Obch9J3F41cv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iJqwdLsTjfti"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hI10Doxilc-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Read Any File Type"
      ],
      "metadata": {
        "id": "OxIrZVZ15DQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWdEhQ61jdpP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_documents(file_path: str):\n",
        "    \"\"\"\n",
        "    Loads the content of .txt, .csv, .jsonl, or .pdf files and returns a list of text documents.\n",
        "    Perfect for building a RAG corpus.\n",
        "    \"\"\"\n",
        "    file_path = Path(file_path)\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    ext = file_path.suffix.lower()\n",
        "\n",
        "    # TXT\n",
        "    if ext == \".txt\":\n",
        "        return [file_path.read_text(encoding=\"utf-8\")]\n",
        "\n",
        "    # CSV (first column = content)\n",
        "    elif ext == \".csv\":\n",
        "        df = pd.read_csv(file_path)\n",
        "        return df.iloc[:, 0].astype(str).tolist()\n",
        "\n",
        "    # JSONL (each line = {\"text\": \"...\"} )\n",
        "    elif ext == \".jsonl\":\n",
        "        docs = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                docs.append(json.loads(line).get(\"text\", \"\"))\n",
        "        return docs\n",
        "\n",
        "    # PDF\n",
        "    elif ext == \".pdf\":\n",
        "        reader = PdfReader(str(file_path))\n",
        "        pages = [page.extract_text() for page in reader.pages]\n",
        "        return [p for p in pages if p]   # remove empty pages\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {ext}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfSlS65nknV2"
      },
      "outputs": [],
      "source": [
        "# Load our Corpus\n",
        "\n",
        "file_path = \"/content/Big_Data.pdf\"  # source pdf that will be use as a RAG knowledge base\n",
        "corpus_texts = load_documents(file_path)\n",
        "\n",
        "print(f\"Loaded {len(corpus_texts)} documents from {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Model\n",
        "We use `all-MiniLM-L6-v2`, a fast and lightweight embedding model ideal for retrieval"
      ],
      "metadata": {
        "id": "QBZG7_IN5rAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-8F7INKkrl_"
      },
      "outputs": [],
      "source": [
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJPfX5rTkuLi"
      },
      "outputs": [],
      "source": [
        "# Convert all documents into dense vectors and normalize them for FAISS retrieval (Embeddings).\n",
        "\n",
        "print(\"\\n Embedding corpus...\")\n",
        "\n",
        "corpus_embeddings = embed_model.encode(\n",
        "    corpus_texts,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "# FAISS requires normalized vectors for cosine similarity\n",
        "faiss.normalize_L2(corpus_embeddings)\n",
        "\n",
        "dim = corpus_embeddings.shape[1]\n",
        "print(f\"Vector dimension: {dim}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build FAISS Index\n"
      ],
      "metadata": {
        "id": "VsvIRogT6zAi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbWBp86zkzuM"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Building FAISS index...\")\n",
        "\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "print(f\" FAISS index built with {index.ntotal} vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask a Query â†’ Retrieve Top-K\n"
      ],
      "metadata": {
        "id": "ki3NxG5v7wNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdtWsSHdk5S7"
      },
      "outputs": [],
      "source": [
        "query = \"What are the charactristcs of big data? \"\n",
        "print(\"\\n Your query:\", query)\n",
        "\n",
        "# Embed query\n",
        "q_vec = embed_model.encode(query, convert_to_numpy=True).reshape(1, -1)\n",
        "faiss.normalize_L2(q_vec)\n",
        "\n",
        "# FAISS retrieve\n",
        "scores, idx = index.search(q_vec, k=5)\n",
        "\n",
        "retrieved_docs = []\n",
        "for rank, doc_i in enumerate(idx[0]):\n",
        "    retrieved_docs.append({\n",
        "        \"rank\": rank + 1,\n",
        "        \"text\": corpus_texts[doc_i]\n",
        "    })\n",
        "\n",
        "print(\"\\nTop Retrieved Docs:\")\n",
        "display(pd.DataFrame(retrieved_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Rerank with Cross Encoder (Better Precision)\n"
      ],
      "metadata": {
        "id": "udFVmZsj7_Wv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HodcZImzk78N"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Reranking using CrossEncoder...\")\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "pairs = [[query, d[\"text\"]] for d in retrieved_docs]\n",
        "rerank_scores = reranker.predict(pairs)\n",
        "\n",
        "# Attach rerank score\n",
        "for i, score in enumerate(rerank_scores):\n",
        "    retrieved_docs[i][\"rerank_score\"] = float(score)\n",
        "\n",
        "# Sort by cross-encoder\n",
        "reranked = sorted(retrieved_docs, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "print(\"\\n Top Reranked Docs:\")\n",
        "display(pd.DataFrame(reranked))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Final RAG Prompt\n",
        "Assemble the top retrieved documents and generate a final instruction prompt for the LLM.\n"
      ],
      "metadata": {
        "id": "pAlVV5kW8J7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUXLOBQNk98C"
      },
      "outputs": [],
      "source": [
        "top_docs = reranked[:3]  # use top-3 for LLM\n",
        "\n",
        "context = \"\\n\\n\".join(\n",
        "    [f\"[Doc {i+1}] {d['text']}\" for i, d in enumerate(top_docs)]\n",
        ")\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Use ONLY the following evidence:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer clearly, scientifically, and with accurate facts.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n Final Prompt Sent to LLM:\\n\")\n",
        "print(prompt[:600], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Final Answer"
      ],
      "metadata": {
        "id": "u80Lr5dL8oHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Phi-3.5-mini-instruct (fast, small, reliable) to produce the final RAG answer\n",
        "\n",
        "print(\"\\n Generating answer...\")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
        "    max_new_tokens=200,\n",
        "    device=device # Explicitly set device to use GPU if available, otherwise CPU\n",
        ")\n",
        "\n",
        "answer = generator(prompt)[0][\"generated_text\"]\n",
        "\n",
        "print(\"\\n Final Answer:\\n\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "97G1am6uEacx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}